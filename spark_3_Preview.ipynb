{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark_3_Preview",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xvanausloos/DevSpark_Labs/blob/master/spark_3_Preview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwqKU-PhhC58"
      },
      "source": [
        "#[Apache Spark 3.0.0 with Google Colab](http://apache.osuosl.org/spark/spark-3.0.0/)\n",
        "\n",
        "**Author: MA Raza**\n",
        "\n",
        "This is the working google collaboratory notebook example of setting up  recently release spark 3.0.0 in google colab. \n",
        "\n",
        "* Installing Java in the Google Colaboratory\n",
        "* Setting up Spark 3.0 in the Google Colaboratory\n",
        "* A test example\n",
        "\n",
        "**References:**\n",
        "\n",
        "1. http://apache.osuosl.org/spark/spark-3.0.0/\n",
        "2. https://medium.com/@sushantgautam_930/apache-spark-in-google-collaboratory-in-3-steps-e0acbba654e6\n",
        "3. https://notebooks.gesis.org/binder/jupyter/user/databricks-koalas-kuv5qckt/notebooks/docs/source/getting_started/10min.ipynb\n",
        "4. https://medium.com/@amjadraza24/getting-started-spark3-0-0-with-google-colab-9796d350d78\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2COZzvqcf-Pg"
      },
      "source": [
        "# Setting up Spark 3.0.0 in the Google Colaboratory\n",
        "\n",
        "This notebook comprises the instructions to run Sprak and pyspark in google colabboratory framework. \n",
        "\n",
        "We will be running the excises of  some basic functions on using koalas\n",
        "\n",
        "We will install below programs\n",
        "\n",
        "* Java 8\n",
        "* spark-3.0.0\n",
        "* Hadoop3.2 \n",
        "* [Findspark](https://github.com/minrk/findspark)\n",
        "\n",
        "\n",
        "This installs Apache Spark 3.0.0, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark.\n",
        "\n",
        "Run below set of commands to install spark. \n",
        "\n",
        "**Change the location/version of spark version of your choice**\n",
        "\n",
        "**[Apache Spark in Google Collaboratory In 3 steps.](https://medium.com/@sushantgautam_930/apache-spark-in-google-collaboratory-in-3-steps-e0acbba654e6)**\n",
        "\n",
        "> Make sure the spark-version you are downloading is availbale on target link\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MZdDu9LhLwN",
        "outputId": "fbd38668-79d7-4716-eb8d-a43f5f5a3c23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Run below commands\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tar: spark-3.0.0-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKbDZzlDe_aT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e1729e0e-b451-4bb0-e2b6-dc2f060fdcc1"
      },
      "source": [
        "!ls -a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".   .config\t spark-3.0.0-bin-hadoop3.2\n",
            "..  sample_data  spark-3.0.0-bin-hadoop3.2.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sit9TX0chlNF"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da7M2Wi1jC1c"
      },
      "source": [
        "## Spark Installation test\n",
        "Lets test the installation of spark in our google colab environment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y18KVg34jkXj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "9045c25c-af52-483f-9983-8faeaf988ff9"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Test the spark \n",
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "\n",
        "df.show(3, False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/session.py:378: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe6Mlcs29vX-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "7670561b-6a0b-4be0-92ff-4084f892c347"
      },
      "source": [
        "# Check the pyspark version\n",
        "import pyspark\n",
        "print(pyspark.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjUld8S6bRHz"
      },
      "source": [
        "# Conclusions\n",
        "\n",
        "In this notebook, we learned\n",
        "\n",
        "* Running spark 3.0.0-preview2 in Google Colab\n",
        "* Running pysprak3.0.0dev2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At0dtijv-aBn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}